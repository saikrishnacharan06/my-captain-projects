#import dependencies
import numpy
import nltk
nltk.download('stop words')
import sys
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense,Dropout,LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint
#load data
file=open('frankenstein-2.txt').read()
#tokenization
def tokenize_words(input):
 input=input.lower
 tokenizer=RegexpTokenizer(r'\w+')
 tokens=tokenizer.tokenize(input)
 filtered=filter(lambda token: token not in stopwords.words('english'),tokens)
 return"".join(filtered)
processed_inputs=tokenize_words(file)
#characters to numbers
chars=(sorted(list(set(processed_inputs)))
char_to_num=dict((c,i) for i,c in enumerate(chars))
#check if this has worked or otherwise
input_len=len(processed_inputs)
vocab_len=len(chars)
print('total no of chars',input_len)
print('total vocab',vocab_len)
#seq length
seq_length=100
x_data=[]
y_data=[]
#loop through the sequence
for i in range(0,input_len-seq_length,1):
 in_seq=processed_inputs[i:i + seq_length]
 out_seq=processed_inputs[i + seq_length]
 x_data.append([char_to_num[char] for char in in_seq])
 y_data.append([char_to_num[out_seq])
n_patterns=len(x_data)
print('total patterns',n_patterns)
#convert input sequence to np array
X=numpy.reshape(x_data,(n_patterns,seq_length,1))
X=X/float(vocab_len)
#one hot encoding
y=np_utils.to_categorical(y_data)
#creating the model
model=Sequencial()
model.add(LSTM(256,input_shape=(X_shape[1],X_shape[2],return_sequences=True)
model.add(Dropout(0.2))
model.add(LSTM(256,return_sequences=True)
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1],activation='softmax'))
#compile the model
model.compile(loss='categorical_crossentropy',optimizer='adam')
#saving weights
filepath='model_weights_saved.hdf5'
checkpoint=ModelCheckpoint(filepath,monitors='loss',verbose=1,save_best_only=True,mode='min')
desired_callbacks=[checkpoint]
#fit model and let it train
model.fit(X,y,epochs=4,batch_size=256,callbacks=desired_callbacks
#recomplile model with saved weights
filename='model_weights_saved.hdf5'
model.load_weights(filename)
model.compile(loss='categorical_crossentropy',optimizer='adam')
#output back into characters
num_to_char=dict((i,c) in enumerate(chars))
#random seed to help generate
start=numpy.random.randit(0,len(x_data)-1)
pattern=x_data(start)
print('random seed:')
print("\"",''.join([num_to_char[value] for value in pattern]),"\"")
#generate te text
for i in range(1000):
 x=numpy.reshape(pattern,(1,len(pattern),1))
 x=x/float(vocab_len)
 
